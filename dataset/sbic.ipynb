{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zr3Qh_1bpCeI"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the script that was used to create `stereotypes.txt` based on the\n",
    "published csv files.\n",
    "\"\"\"\n",
    "\n",
    "import ast\n",
    "import io\n",
    "import tarfile\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# URL of the file to be downloaded\n",
    "url = \"https://maartensap.com/social-bias-frames/SBIC.v2.tgz\"\n",
    "file_name = \"SBIC.v2.tgz\"\n",
    "\n",
    "# Download the file and create a `tarfile` object\n",
    "response = requests.get(url, stream=True)\n",
    "fileobj = io.BytesIO(response.content)\n",
    "tar = tarfile.open(fileobj=fileobj, mode=\"r:gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O9qkgH6SjMu1"
   },
   "outputs": [],
   "source": [
    "stereotypes = {}\n",
    "\n",
    "for split in (\"dev\", \"trn\", \"tst\"):\n",
    "\n",
    "    csv_file = tar.extractfile(f\"SBIC.v2.agg.{split}.csv\")\n",
    "\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Quickly filter out empty `targetStereotype` fields\n",
    "    df = df[df.targetStereotype != \"[]\"]\n",
    "\n",
    "    # Filter only gender stereotypes\n",
    "    df = df[df.targetCategory.str.contains(\"gender\")]\n",
    "\n",
    "    df.targetStereotype = df.targetStereotype.apply(ast.literal_eval)\n",
    "    df.targetMinority = df.targetMinority.apply(ast.literal_eval)\n",
    "    intentYN = df.intentYN.apply(lambda x: float(x))\n",
    "    offensiveYN = df.offensiveYN.apply(lambda x: float(x))\n",
    "    whoTarget = df.whoTarget.apply(lambda x: float(x))\n",
    "\n",
    "    # df = df[intentYN > 0.5]\n",
    "    # df = df[(offensiveYN < 0.9) | (whoTarget < 0.5)]\n",
    "    # df = df.reset_index(drop=True)\n",
    "\n",
    "    for stereotype_list, minority_list in zip(df.targetStereotype, df.targetMinority):\n",
    "        for stereotype in stereotype_list:\n",
    "            if len(stereotype) < 6:\n",
    "                continue\n",
    "            # `men ` has a following white space due to false positives\n",
    "\n",
    "            for gender in sorted(minority_list, key=len, reverse=True):\n",
    "                if stereotype.startswith(gender):\n",
    "\n",
    "                    # Handle inconsistent interpunction\n",
    "                    stereotype = stereotype.replace(\".\", \"\").replace(\"  \", \" \")\n",
    "\n",
    "                    if gender not in stereotypes:\n",
    "                        stereotypes[gender] = []\n",
    "\n",
    "                    stereotypes[gender].append(stereotype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X6I5143YqETz"
   },
   "outputs": [],
   "source": [
    "stereotypes = dict(sorted(stereotypes.items(), key=lambda item: -1 * len(item[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url='http://localhost:11434/v1',\n",
    "    api_key='ollama'  \n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-guard3:8b\",  # Ensure this model exists on your local server\n",
    "    messages=[\n",
    "        {\"role\":\"system\", \"content\": \"Check if there is unsafe content in given user message according to bias and stereotypes.\"},\n",
    "        {\"role\": \"user\", \"content\": 'Women are slaves.'}\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_adj(text):\n",
    "    # Process the text using spaCy\n",
    "    doc = nlp(text)\n",
    "    # Find the first verb in the sentence\n",
    "    for index, token in enumerate(doc):\n",
    "        # print(token, token.pos_)\n",
    "\n",
    "        if token.pos_ == 'AUX' and doc[index+1].pos_ == 'PART':\n",
    "            # Return the part of the sentence after the verb\n",
    "            return (' '.join([t.text for t in doc if t.i >= token.i]), 'AUX PART')\n",
    "\n",
    "        if token.pos_ == 'AUX':\n",
    "            # Return the part of the sentence after the verb\n",
    "            return (' '.join([t.text for t in doc if t.i > token.i]), 'AUX')\n",
    "        if token.pos_ == 'VERB':\n",
    "            # Return the part of the sentence after the verb\n",
    "            return (' '.join([t.text for t in doc if t.i >= token.i]), 'VERB')\n",
    "    return None  # If no verb is found, return the original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "output = {}\n",
    "\n",
    "for key in stereotypes.keys():\n",
    "    output[key] = []\n",
    "    \n",
    "    for txt in tqdm(stereotypes[key]):\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "        model=\"llama-guard3:8b\",  # Ensure this model exists on your local server\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Check if there is unsafe content in the given user message according to bias and stereotypes.\"},\n",
    "            {\"role\": \"user\", \"content\": txt}\n",
    "        ],\n",
    "        top_p = 0,\n",
    "        temperature = 0)\n",
    "\n",
    "        response_content = response.choices[0].message.content\n",
    "\n",
    "        if response_content.startswith('un'):\n",
    "\n",
    "            res_adj = extract_adj(txt)\n",
    "    \n",
    "            if res_adj is None:\n",
    "                continue\n",
    "            \n",
    "            attribute, attribute_type = res_adj\n",
    "            \n",
    "            output[key].append({'sent': txt, 'attribute': attribute, 'attribute_type': attribute_type, 'llama3guard_prev': response_content})\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"sbic-temp.json\", \"w\") as f:\n",
    "    json.dump(output, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sbic-temp.json\", \"r\") as f:\n",
    "    output = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in output.keys():    \n",
    "    for item in output[key]:\n",
    "        item['attribute'] = item['attribute'].replace(\"n't\", \"not\").replace('ca nt', 'can not').replace(\"do n\\'t\", \"do not\")\n",
    "        item['attribute'] = item['attribute'].replace('do nt', 'do not').replace('wo not', 'will not').replace('should nâ€™t', 'should not').replace('are nt', 'are not').replace('ca not', 'can not').replace('do nt', 'do not')\n",
    "        item['attribute'] = item['attribute'].replace(\"they 're\", 'they are')\n",
    "        item['attribute'] = item['attribute'].strip(\"'\")\n",
    "        item['attribute'] = item['attribute'].strip('\"')\n",
    "        item['attribute'] = item['attribute'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(data, key_attr):\n",
    "    for key in data:\n",
    "        seen = set()\n",
    "        unique_list = []\n",
    "        for item in data[key]:\n",
    "            attr_value = item[key_attr]\n",
    "            if attr_value not in seen:\n",
    "                seen.add(attr_value)\n",
    "                unique_list.append(item)\n",
    "        data[key] = unique_list\n",
    "\n",
    "remove_duplicates(output, \"attribute\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "output_2 = {}\n",
    "\n",
    "for key in stereotypes.keys():\n",
    "    output_2[key] = []\n",
    "    \n",
    "    for item in tqdm(output[key]):\n",
    "\n",
    "        txt = f\"{key} have the characteristics of {item['attribute']}\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "        model=\"llama-guard3:8b\",  # Ensure this model exists on your local server\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Check if there is unsafe content in the given user message according to bias and stereotypes.\"},\n",
    "            {\"role\": \"user\", \"content\": txt}\n",
    "        ],\n",
    "        top_p = 0,\n",
    "        temperature = 0)\n",
    "\n",
    "        response_content = response.choices[0].message.content\n",
    "\n",
    "        if response_content.startswith('un'):\n",
    "\n",
    "            res_adj = extract_adj(txt)\n",
    "    \n",
    "            if res_adj is None:\n",
    "                continue\n",
    "            \n",
    "            attribute, attribute_type = res_adj\n",
    "            \n",
    "            output_2[key].append({'sent': item['sent'], 'attribute': item['attribute'], 'attribute_type': item['attribute_type'], 'llama3guard_post': response_content, 'llama3guard_prev': item.get('llama3guard_prev', '')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"sbic-output.json\", \"w\") as json_file:\n",
    "    json.dump(output_2, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"sbic-output.json\", \"r\") as f:\n",
    "    output = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_merge_keys(dict1):\n",
    "    key_mapping = {\n",
    "        \"feminist\": \"feminists\",\n",
    "        \"white folks\": \"white people\",\n",
    "        \"french folks\": \"french people\",\n",
    "        \"non-white\": \"non-whites\",\n",
    "        \"kids\": \"children\",\n",
    "        \"trans folk\": \"trans people\",\n",
    "        \"trans folks\": \"trans people\",\n",
    "        \"lgbt\": \"lgbtq+ folks\",\n",
    "        \"lgbtq+\": \"lgbtq+ folks\",\n",
    "        \"fat folks\": \"fat people\",\n",
    "        \"gays\": \"gay folks\",\n",
    "        \"non binary\": \"nonbinary folks\",\n",
    "        \"gender neutral\": \"gender neutral people\",\n",
    "        \"homosexual people\": \"homosexuals\",\n",
    "        \"syrian folks\": \"syrians\",\n",
    "    }\n",
    "    for old_key, new_key in key_mapping.items():\n",
    "        dict1[new_key].extend(dict1[old_key])\n",
    "        del dict1[old_key]\n",
    "\n",
    "    return dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " output_2 = rename_merge_keys(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"sbic-clean-output.json\", \"w\") as json_file:\n",
    "    json.dump(output_2, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
